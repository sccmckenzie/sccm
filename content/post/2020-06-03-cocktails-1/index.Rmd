---
title: 'Cocktails: Experimenting with Cosine String Distance'
author: Scott McKenzie
date: '2020-06-03'
slug: cocktails-1
tags:
  - R
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, dev = "svglite")
```


You suddenly discover that you are hosting a party tonight. The guests are expecting you to prepare cocktails. With limited time to prepare, you can only make one trip to the store. What ingredients should you buy in order to maximize your mixological palette? Assume your shopping cart holds `n` items.

In Part 1, we will introduce the `cocktails` dataset, which forms the inspiration for this problem. Although data cleaning for `cocktails` is mostly done, some touch-ups are needed. We will apply cosine string distance comparisons to eliminate redundant ingredients.

Part 2 focuses on the approach used to optimize ingredient selection.

Click <a href = "https://sccm.shinyapps.io/cocktail-ingredients/" target = "_blank">here</a> to see the Shiny app that illustrates the final results!

# Background

The [`cocktails`](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-26/readme.md) dataset comes from TidyTuesday[^1].
```{r message = FALSE}
library(tidyverse)

# Getting data from TidyTuesday repo
cocktails <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/cocktails.csv') %>% 
  select(drink, ingredient_number, ingredient, measure) 
```

```{r message=FALSE, include = FALSE}
# some examples...
set.seed(16)
drink_sample <- cocktails %>% 
  distinct(drink) %>% 
  sample_n(3)

n_original <- n_distinct(cocktails$ingredient)
```

`cocktails` contains `r nrow(distinct(cocktails, drink))` unique drinks. I never considered myself a proper connoisseur by any means, but this dataset serves as a humbling realization that I've barely scratched the surface of mixology: `r paste0(drink_sample$drink, collapse = ", ")`, to name a few. 

The data is structured as shown below. Each ingredient + associated measure is stored in a unique row.
```{r echo = FALSE}
library(reactable)

cocktails %>% 
  # filtered to dirty martini for succinct example
  filter(drink == "Dirty Martini") %>% 
  reactable(columns = list(
    drink = colDef(width = 125),
    ingredient_number = colDef(width = 175)
  ))
```

The plot below shows the most frequently used ingredients in `cocktails`. At first glance, there are $`r nrow(distinct(cocktails, ingredient))`$ distinct ingredients.

Vodka & gin take gold & silver, but I was surprised to see that whiskey doesn't even make the top 8! Otherwise, the below charts seems to make sense.

<br><br>
```{r echo=FALSE}
cocktails %>% 
  count(ingredient) %>% 
  mutate(n = n / n_distinct(cocktails$drink)) %>% 
  slice_max(n, n = 8) %>% # new dplyr 1.0.0 function :)
  mutate(ingredient = fct_reorder(ingredient, n)) %>% 
  ggplot(aes(n, ingredient)) +
  geom_col(fill = "#a5c261") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        plot.title.position = "plot",
        text = element_text(size = 15)) +
  scale_x_continuous(expand = expansion(0),
                     labels = scales::percent_format(1),
                     position = "top") +
  labs(y = "",
       x = "",
       title = "Top Cocktail Ingredients",
       subtitle = "% Occurrence")
```

With just these ingredients, the only drink you'd be able to make to order is a Screwdriver (Vodka + Orange Juice). If you're trying impress company, this list is not comprehensive enough - your first guest who orders an Old-Fashioned will be disappointed.

Is the solution to simply buy *more* ingredients? We are assuming that going down the ingredient count list is the smartest method to solve this problem. Spoiler alert: it's not (more in part 2).

<!-- In order to expand our capability for a wider variety of drinks, we need an effective way to visualize the most common combinations of ingredients. Relying on simple bar plots would quickly become impractical. -->

<!-- I've seen a few examples from other contributors involving network plots[^2] - a great way to visualize the interconnectedness between the `r nrow(distinct(cocktails, drink))` drinks. -->

# The First Roadblock

Skimming through the distinct ingredients, we can observe duplicates due to small differences in capitalization and superfluous adjectives. This could add unwanted redundancy to our selection method and unnecessarily convolute our network plot.

After all, how many different types of lime juice do we need?

```{r}
cocktails %>% 
  distinct(ingredient) %>% 
  filter(str_detect(ingredient, "(L|l)ime"))
```

The first thing we can do is force everything to lowercase and remove punctuation.

```{r message = FALSE}
cocktails <- cocktails %>% 
   mutate(ingredient = str_to_lower(ingredient),
         ingredient = str_remove(ingredient, "'"))
```

Although this is a good start, we really need a systematic way to detect similar ingredients. Making $`r n_original` ^ 2 /2 = `r n_original ^ 2 /2`$ manual comparisons is unrealistic.

Hence, a perfect opportunity to use string distance comparisons. I highly recommend M van der Loo's paper[^3] which provides a thorough introduction to the [stringdist](https://github.com/markvanderloo/stringdist) package. We will use the [fuzzyjoin](http://varianceexplained.org/fuzzyjoin/) package which harnesses the stringdist tools in a user-friendly interface; but we need to be careful to use a targeted approach.

For example, `lime peel` and `lime juice` both contain `lime` and by many string distance metrics, will be fairly similar. But we certainly don't want to merge `lime juice` and `lime peel` into a single ingredient.

We can perform more meaningful comparisons by first separating the ingredients into categories. This is an excellent opportunity to use the [tokenizers](https://docs.ropensci.org/tokenizers/) package + new [dplyr 1.0.0](https://dplyr.tidyverse.org/) functions!

I'm a huge fan of `with_groups()` since it combines grouping + action + ungroup into one operation, making code chunks more concise.

```{r}
library(tokenizers)
with_categories <- cocktails %>% 
  distinct(ingredient) %>% 
  with_groups(ingredient,
              summarise,
              category = tokenize_regex(ingredient,
                                        # essentially word tokenization
                                        pattern = " ",
                                        simplify = TRUE)) %>% 
  add_count(category, name = "freq") %>% 
  # within each ingredient, what are the most common words?
  with_groups(ingredient, slice_max, n = 1, order_by = freq) %>%
  with_groups(ingredient, filter, n() == 1) %>% 
  filter(freq > 6)
```

This results in the below categories.[^4]

```{r echo=FALSE}
with_categories %>% 
  count(category, sort = TRUE) %>% 
  reactable(fullWidth = FALSE)
```
<br>
We can now apply string distance matching within each category. Consider the juice category as an example. We have the following juices:

```{r echo=FALSE}
with_categories %>% 
  filter(category == "juice") %>% 
  .$ingredient
```

We want to evaluate the string distance between each one of these elements. Since these are all juices, we can omit "juice". Let's also omit white-space.

```{r echo=FALSE}
juices <- with_categories %>% 
  filter(category == "juice") %>% 
  mutate(reduced = str_remove(ingredient, category) %>% 
              str_remove_all(" ")) %>% 
  .$reduced

with_categories %>% 
  filter(category == "juice") %>% 
  transmute(before = ingredient,
            after = juices) %>% 
  sample_n(5) %>% 
  reactable(fullWidth = FALSE,
            defaultColDef = colDef(width = 150),
            columns = list(
              after = colDef(style = list(background = "#2F8475", color = "white"))
            ))
```
<br>
I opted to use q-gram cosine as distance metric, with $q = 1$. I experimented with edit-based methods, but q-gram seemed to perform best on this dataset.

Suppose we are comparing `freshlime` and `lime`. With q-gram cosine, we first construct a set of all possible unique q-length strings (in this case, $length = 1$) between the two elements, yielding the following set:
$$set: [f, r, e, s, h, l, i, m]$$
Easy enough. Now, we assign a vector to each element that corresponds to the above set.
$$v(freshlime) = [1, 1, 2, 1, 1, 1, 1, 1]\\v(lime) = [0,0,1,0,0,1,1,1]$$
Now, we simply take the dot product of the two vectors and subtract from one.
$$1 - \frac{2 + 1 + 1 + 1}{\sqrt{11}\sqrt{4}} = 0.246$$
A value of $0$ indicates an exact match (vectors are identical), whereas a value of $1$ indicates no q-gram commonality (perpendicular vectors).

A q-gram cosine matrix for a few juices is shown below. `freshlime` and `lime` share the lowest value in this set - which is what we intended.

```{r echo=FALSE}
clr <- function(x) rgb(colorRamp(c("#79D7F7", "#F5F5F5"))(x), maxColorValue = 255)

mat <- stringdist::stringdistmatrix(juices[c(2, 4, 8, 10, 15)], juices[c(2, 4, 8, 10, 15)], method = "cosine", q = 1, useNames = "strings")

mat %>% 
  reactable(
    compact = TRUE,
    defaultColDef = colDef(
      style = function(value) {
        normalized <- (value - min(mat)) / (max(mat) - min(mat))
        color <- clr(normalized)
        if (value == 0) return(list(background = "white")) else return(list(background = color))
      },
      cell = function(value) {
        if (!is.numeric(value)) return(value)
        if (value == 0) return("x") else return(round(value, 2))
      }
    ),
    columns = list(
    .rownames = colDef(style = "font-weight: bold;")
  ))
```

<br>
I set the lower string distance threshold to $0.3$ - anything below this value will be held out for review.

```{r message=FALSE, include = FALSE}
library(fuzzyjoin)
ingredients_similar <- with_categories %>% 
  transmute(category,
            ingredient,
            # e.g. "fresh lime juice" -> "freshlime"
            reduced = str_remove(ingredient, category) %>% 
              str_remove_all(" ")) %>% 
  # remove ingredients that are synonymous with category
  filter(reduced != "") %>% 
  group_nest(category) %>%
  rowwise() %>% 
  summarise(category = category,
            stringdist_left_join(data,
                                 data, 
                                 by = "reduced",
                                 method = "cosine",
                                 q = 1,
                                 distance_col = "dist") %>%
              filter(dist < 0.3,
                     #identical strings have distance of 0
                     ingredient.x != ingredient.y) %>%
              # trick to remove mirrored rows
              distinct(dist, .keep_all = TRUE)) %>% 
  arrange(dist)
```


The resulting table is below! Although there are quite a few false positives, we've effectively reduced a list of $`r nrow(distinct(cocktails, ingredient))` ^ 2 /2$ combinations to $`r nrow(ingredients_similar)`$ rows for our review.

(I've highlighted the rows I've chosen to merge)
```{r echo=FALSE}
highlight <- function(value, cellInfo) {
  if(cellInfo %in% c(3, 6, 8, 9, 10, 11, 13)) return(list(background = "#FFC97F"))
}

ingredients_similar %>% 
  select(!contains("reduced")) %>% 
  arrange(dist) %>% 
  reactable(
    columns = list(
      dist = colDef(
        style = function(value) {
          normalized <- (value - min(ingredients_similar$dist)) / (max(ingredients_similar$dist) - min(ingredients_similar$dist))
          color <- clr(normalized)
          if (value == 0) return(list(background = "white")) else return(list(background = color))
        },
        format = colFormat(digits = 2)
      ),
      category = colDef(
        width = 90,
        style = function(value, cellInfo) {
          highlight(value, cellInfo)
        }
      ),
      ingredient.x = colDef(
        width = 240,
        style = function(value, cellInfo) {
          highlight(value, cellInfo)
        }
      ),
      ingredient.y = colDef(
        width = 200,
        style = function(value, cellInfo) {
          highlight(value, cellInfo)
        }
      )
    )
  )
```



```{r include = FALSE}
ingredients_replace <- ingredients_similar %>% 
  slice(c(3, 6, 8, 9, 10, 11, 13)) %>% 
  select(contains("ingredient")) %>% 
  rowwise() %>% 
  mutate(ingredient = list(c(ingredient.x, ingredient.y))) %>% 
  transmute(old_ingredient = ingredient[which.max(str_length(ingredient))],
            new_ingredient = ingredient[which.min(str_length(ingredient))])

cocktails <- cocktails %>% 
  left_join(with_categories) %>% 
  left_join(ingredients_replace, by = c("ingredient" = "old_ingredient")) %>% 
    mutate(category = coalesce(category, "other"),
           ingredient = if_else(is.na(new_ingredient), ingredient, new_ingredient)) %>% 
  select(-new_ingredient)
```

# Conclusion

Altogether, the various procedures above have reduced the distinct ingredients from $`r n_original`$ to $`r n_distinct(cocktails$ingredient)`$. Click [here](https://github.com/sccmckenzie/cocktail-ingredients/blob/master/data-clean.R) to see data transformation code from start to finish.[^5]

In Part 2, we will return to the original problem:

What ingredients should you buy in order to maximize your mixological palette?


[^1]: [`TidyTuesday`](https://github.com/rfordatascience/tidytuesday) is a weekly project hosted by the `R4DS Online Learning Community` that encourages beginner & seasoned data scientists to apply their skills on a variety of datasets.

[^2]: [https://twitter.com/CedScherer/status/1266434219527585799](https://twitter.com/CedScherer/status/1266434219527585799)<br>[https://twitter.com/jakekaupp/status/1266332219565576192](https://twitter.com/jakekaupp/status/1266332219565576192)<br>[https://twitter.com/AndreaNOdell/status/1266198470798401536](https://twitter.com/AndreaNOdell/status/1266198470798401536)

[^3]: van der Loo M (2014). “The stringdist package for approximate string matching.” _The R Journal_,
*6*, 111-122. <URL: https://CRAN.R-project.org/package=stringdist>.

[^4]: Keywords that contain a global frequency of less than 6 remain unclassified.

[^5]: If I had more time, I would like to run the string distance comparison on all uncategorized ingredients.